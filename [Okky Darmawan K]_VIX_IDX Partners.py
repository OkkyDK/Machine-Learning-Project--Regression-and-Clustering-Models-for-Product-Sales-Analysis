# -*- coding: utf-8 -*-
"""[Okky Darmawan K]_VIX_IDX Partners.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mss_ZaWLrDV1i3C4xTlqbmaBLDxi04Vf

# Import Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

print(pd.__version__)
print(np.__version__)
print(sns.__version__)

"""# Load Dataset"""

df = pd.read_csv('loan_data_2007_2014.csv',low_memory=False)

# Mengatur opsi display.max_columns ke None
pd.set_option('display.max_columns', None)

df.sample(5)

"""# Exploratory Data Analysis

## Descriptive Analytics
"""

df.info()

df['purpose'].unique()

# Counting the number of unique values in each column
unique_counts = df.nunique()

# Applying filtering to columns with less than 10 unique values
filtered_columns = unique_counts[(unique_counts < 10) & (unique_counts > 0)]

# Displaying the number of unique values for the filtered columns
filtered_columns

# Applying the filter to columns with fewer than 10 unique values
filtered_columns = filtered_columns.index.tolist()

# Displaying the unique values of each column in filtered_columns
for column in filtered_columns:
    unique_values = df[column].unique()
    print(f"Unique values of column '{column}':")
    print(unique_values)
    print()

# Separating columns based on data types
object_cols = df[filtered_columns].select_dtypes(include=['object']).columns
numeric_cols = df[filtered_columns].select_dtypes(include=['int', 'float']).columns

print('Object columns:\n', list(object_cols), '\n')
#print('Date columns:\n', list(date), '\n')
print('Numeric columns:\n', list(numeric_cols))

# Statistical description of categorical columns
df[object_cols].describe()

df.describe()

"""## Univariate Analysis"""

# Determining the number of rows and columns for subplots
num_plots = len(object_cols)
num_cols = 2
num_rows = (num_plots + num_cols - 1) // num_cols

# Creating subplots
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))

# Setting up a counter for plots
counter = 0

# Creating horizontal count plots for each object column
for row in range(num_rows):
    for col in range(num_cols):
        if counter < num_plots:
            col_name = object_cols[counter]
            sns.countplot(data=df, y=col_name, ax=axes[row, col], orient='h')
            axes[row, col].set_title(f"Count Plot - {col_name}")
            counter += 1
        else:
            # If the counter exceeds the number of object columns, remove the remaining subplot
            fig.delaxes(axes[row, col])

# Adjusting the layout of subplots
plt.tight_layout()

# Displaying the plot
plt.show()

# 15 Choosen Columns
selected_columns = ['loan_status','loan_amnt', 'int_rate', 'installment', 'grade', 'home_ownership', 'annual_inc', 'verification_status', 'dti', 'delinq_2yrs', 'total_acc', 'revol_bal', 'pub_rec', 'emp_length', 'collections_12_mths_ex_med']

selected_df = df[selected_columns]
selected_df.info()

# Create a pair plot of all features based on the cluster results
sns.pairplot(df, vars=num_list, hue='loan_status')
plt.show()

# Calculate the number of rows and columns for the subplot layout
n_rows = len(num_list) // 4 + (len(num_list) % 4 > 0)
n_cols = min(len(num_list), 4)

# Create histograms for each column with a 4-column layout
fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 15))

# Determine the number of unused subplots
unused_axes = n_rows * n_cols - len(num_list)

# Hide the unused subplots
if unused_axes > 0:
    for i in range(unused_axes):
        axes[-1, -i-1].axis('off')

# Create histograms for each column
for i, column in enumerate(num_list):
    ax = axes[i // n_cols, i % n_cols]
    sns.histplot(data=df, x=column, ax=ax, bins=20,hue='grade', kde=True)
    ax.set_xlabel(column)
    ax.set_ylabel("Frequency")
    ax.set_title("Histogram of {}".format(column))

# Adjust the subplot layout
plt.tight_layout()

# Display the plot
plt.show()

# Calculate the number of rows and columns for the subplot layout
n_rows = len(num_list) // 4 + (len(num_list) % 4 > 0)
n_cols = min(len(num_list), 4)

# Create box plots for each column with a 4-column layout
fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 15))

# Determine the number of unused subplots
unused_axes = n_rows * n_cols - len(num_list)

# Hide the unused subplots
if unused_axes > 0:
    for i in range(unused_axes):
        axes[-1, -i-1].axis('off')

# Create box plots for each column
for i, column in enumerate(num_list):
    ax = axes[i // n_cols, i % n_cols]
    sns.boxplot(data=df, x='grade', y=column, ax=ax)
    ax.set_xlabel("Grade")
    ax.set_ylabel(column)
    ax.set_title("Box Plot of {}".format(column))

# Adjust the subplot layout
plt.tight_layout()

# Display the plot
plt.show()

# Calculate the number of rows and columns for the subplot layout
n_rows = len(num_list) // 4 + (len(num_list) % 4 > 0)
n_cols = min(len(num_list), 4)

# Create violin plots for each column with a 4-column layout
fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 15))

# Determine the number of unused subplots
unused_axes = n_rows * n_cols - len(num_list)

# Hide the unused subplots
if unused_axes > 0:
    for i in range(unused_axes):
        axes[-1, -i-1].axis('off')

# Create violin plots for each column
for i, column in enumerate(num_list):
    ax = axes[i // n_cols, i % n_cols]
    sns.violinplot(data=df, x='grade', y=column, ax=ax)
    ax.set_xlabel("Grade")
    ax.set_ylabel(column)
    ax.set_title("Violin Plot of {}".format(column))

# Adjust the subplot layout
plt.tight_layout()

# Display the plot
plt.show()

"""## Bivariate Analysis"""

num_list = ['loan_amnt', 'int_rate', 'installment', 'annual_inc', 'dti', 'delinq_2yrs', 'total_acc','revol_bal', 'pub_rec', 'collections_12_mths_ex_med']

plt.figure(figsize=(16, 8))

for i in range(0, len(num_list)):
    plt.subplot(4, 3, i+1)
    sns.stripplot(x=df['grade'], y=df[num_list[i]],order=sorted(df['grade'].unique()), palette='Set1')
    plt.xticks(rotation=0)
    plt.xlabel('Grade')
    plt.ylabel(num_list[i])
    plt.tight_layout()

plt.show()

plt.figure(figsize=(16, 8))

for i in range(0, len(num_list)):
    plt.subplot(4, 3, i+1)
    sns.stripplot(y=df['home_ownership'], x=df[num_list[i]],order=sorted(df['home_ownership'].unique()), palette='Set1')
    plt.xticks(rotation=0)
    plt.ylabel('Home_Ownership')
    plt.xlabel(num_list[i])
    plt.tight_layout()

plt.show()

plt.figure(figsize=(16, 8))

for i in range(0, len(num_list)):
    plt.subplot(3, 4, i+1)
    sns.stripplot(x=df['verification_status'], y=df[num_list[i]],order=sorted(df['verification_status'].unique()), palette='Set1')
    plt.xticks(rotation=0)
    plt.xlabel('verification_status')
    plt.ylabel(num_list[i])
    plt.tight_layout()

plt.show()

plt.figure(figsize=(16, 12))

for i in range(0, len(num_list)):
    plt.subplot(6, 2, i+1)
    sns.stripplot(y=df['loan_status'], x=df[num_list[i]],order=sorted(df['loan_status'].unique()), palette='Set1')
    plt.xticks(rotation=0)
    plt.ylabel('loan_status')
    plt.xlabel(num_list[i])
    plt.tight_layout()

plt.show()

# Create a pair plot of all features based on the cluster results
sns.pairplot(df, vars=num_list, hue='loan_status')
plt.show()

"""## Multivariate Analysis"""

plt.figure(figsize=(8,6))
sns.heatmap(df[num_list].corr(), cmap="Greens", annot=True, fmt=' .2f')

"""# Preprocesing

## Missing Values
"""

# 15 Choosen Columns
selected_columns = ['loan_status','int_rate','installment','annual_inc','dti','delinq_2yrs','inq_last_6mths','mths_since_last_delinq','open_acc',
    'revol_util','total_rec_late_fee','collection_recovery_fee','last_pymnt_amnt','mths_since_last_major_derog','tot_coll_amt',
    'tot_cur_bal','total_rev_hi_lim','emp_length', 'collections_12_mths_ex_med']

null_columns = df[selected_columns].columns[df[selected_columns].isnull().any()]
num_null_columns = len(null_columns)

# Display the result
if len(null_columns) > 0:
    print('Number of columns with null values:', num_null_columns)
    print('Kolom-kolom yang memiliki nilai null:', list(null_columns))
else:
    print('Tidak terdapat nilai null pada dataset')

# Null rows
df[null_columns].isna().sum()

# Calculate the percentage of null values in each column
null_percentage = df[null_columns].isnull().sum() / len(df) * 100

# Display the result
print('Percentage of null values in each column:')
null_percentage

"""### Handling Missing Value"""

null_columns = null_columns.drop(['mths_since_last_delinq', 'mths_since_last_major_derog'])

# Null Column Information
df[null_columns].info()

# Numerical Null Column Desctriptive Statistics
df[null_columns].describe()

# Categorical Null Column Desctriptive Statistics
df['emp_length'].describe()

#Imputatation for Numecial Null Columns
num_miss = null_columns.drop('emp_length')
for i in num_miss:
  mean_columns = df[i].mean()
  df[i] = df[i].fillna(mean_columns)

#Imputatation for Categorical Null Columns
cat_miss = ['emp_length']
for i in cat_miss:
  mode_columns = df[i].mode().iloc[0]
  df[i] = df[i].fillna(mode_columns)

#Re-Checking Null Columns
selected_columns = ['loan_status','int_rate','installment','annual_inc','dti','delinq_2yrs','inq_last_6mths','open_acc',
    'revol_util','total_rec_late_fee','collection_recovery_fee','last_pymnt_amnt','tot_coll_amt',
    'tot_cur_bal','total_rev_hi_lim','emp_length', 'collections_12_mths_ex_med']
df[selected_columns].isna().sum()

"""## Duplicates"""

duplicates = df[selected_columns].duplicated()

# Count the number of duplicates
num_duplicates = duplicates.sum()

# Display the result
if num_duplicates > 0:
    print('There are {} duplicate rows in the dataset.'.format(num_duplicates))
else:
    print('There are no duplicates in the dataset.')

"""## Feature Engineering"""

df[selected_columns].info()

"""### Adding New Feature"""

# Create the new column
df['installment_vs_monthly_inc_ratio'] = df['installment']/(df['annual_inc'] / 12)

# Check for IDs with loan_status = Current
duplicate_ids = df[df['loan_status'] == 'Current']['id'].duplicated()

# Display any duplicate IDs
duplicate_ids = duplicate_ids[duplicate_ids == True]
if len(duplicate_ids) > 0:
    print("Duplicate IDs found with loan_status = Current:")
    print(df[df['id'].isin(duplicate_ids)])
else:
    print("No duplicate IDs found with loan_status = Current.")

# Check for member_id with loan_status = Current
duplicate_member_ids = df[df['loan_status'] == 'Current']['member_id'].duplicated()

# Display any duplicate member_ids
duplicate_member_ids = duplicate_member_ids[duplicate_member_ids == True]
if len(duplicate_member_ids) > 0:
    print("Duplicate member_ids found with loan_status = Current:")
    print(df[df['member_id'].isin(duplicate_member_ids)])
else:
    print("No duplicate member_ids found with loan_status = Current.")

# Create a new column 'approval_status' based on the combination of 'grade' and 'loan_status'
df['approval_status'] = 0 #Rejected
df.loc[((df['grade'] == 'A') | (df['grade'] == 'B')) & (df['loan_status'] == 'Fully Paid'), 'approval_status'] = 1 #Approved
df.loc[((df['grade'] == 'A') | (df['grade'] == 'B')) & (df['loan_status'] == 'Current') & df['installment_vs_monthly_inc_ratio'] >= 0.7 , 'approval_status'] = 1 #Approved

# Display the unique values and counts of 'approval_status'
approval_status_counts = df['approval_status'].value_counts()

print(approval_status_counts)

"""### Fearture Encoding"""

# Calculate the quartiles of loan_amnt
quartiles = df['loan_amnt'].quantile([0.25, 0.5, 0.75])

# Define the encoding labels
labels = ['Low', 'Medium', 'High']

# Encode the loan_amnt based on quartiles
df['loan_grade'] = df['loan_amnt'].apply(lambda x: labels[0] if x <= quartiles[0.25] else labels[1] if x <= quartiles[0.5] else labels[2])

# Display the updated DataFrame
df[['loan_amnt', 'loan_grade']].sample(5)

# Calculate the quartiles of loan_amnt
quartiles = df['loan_amnt'].quantile([0.25, 0.5, 0.75])

# Define the encoding values
values = [3, 2, 1]

# Encode the loan_amnt based on quartiles
df['loan_grade_score'] = df['loan_amnt'].apply(lambda x: values[0] if x <= quartiles[0.25] else values[1] if x <= quartiles[0.5] else values[2] )

# Display the updated DataFrame
df[['loan_amnt','loan_grade', 'loan_grade_score']].sample(5)

# Showing unique value in filtered_columns
cat_selected_columns = ['loan_status','grade','home_ownership','emp_length','verification_status']
for column in cat_selected_columns:
    unique_values = df[column].unique()
    print(f"Unique values of column '{column}':")
    print(unique_values)
    print()

# Create a LabelEncoder object
encoder = LabelEncoder()

# Perform label encoding on the 'emp_length' column
df['emp_length_encoded'] = encoder.fit_transform(df['emp_length'])

# Define the desired mapping for label encoding
mapping = {'< 1 year': 0, '1 year': 1, '2 years': 2, '3 years': 3, '4 years': 4,
           '5 years': 5, '6 years': 6, '7 years': 7, '8 years': 8, '9 years': 9,
           '10+ years': 10}

# Map the 'emp_length' column with the defined mapping
df['emp_length_encoded'] = df['emp_length'].map(mapping)

# Display unique values and the corresponding label encoded values
unique_values = df['emp_length'].unique()
encoded_values = df['emp_length_encoded'].unique()

for value, encoded_value in zip(unique_values, encoded_values):
    print(f'{value}: {encoded_value}')

# Mapping the categories
mapping = {
    'Fully Paid': 'Good Debt',
    'Charged Off': 'Bad Debt',
    'Current': 'Good Debt',
    'Default': 'Bad Debt',
    'Late (31-120 days)': 'Bad Debt',
    'In Grace Period': 'Bad Debt',
    'Late (16-30 days)': 'Bad Debt',
    'Does not meet the credit policy. Status:Fully Paid': 'Good Debt',
    'Does not meet the credit policy. Status:Charged Off': 'Bad Debt'
}

# Create a copy of the 'loan_status' column and apply the new classification
df['loan_status_general'] = df['loan_status'].copy()
df['loan_status_general'] = df['loan_status_general'].replace(mapping)

# Display the updated general classification
unique_general_status = df['loan_status_general'].unique()
print(unique_general_status)

# Define the mapping dictionary
mapping = {
    'Fully Paid': 1,
    'Charged Off': 0,
    'Current': 1,
    'Default': 0,
    'Late (31-120 days)': 0,
    'In Grace Period': 0,
    'Late (16-30 days)': 0,
    'Does not meet the credit policy. Status:Fully Paid': 1,
    'Does not meet the credit policy. Status:Charged Off': 0
}

# Create a new column 'loan_status_mapped' with the mapped values
df['loan_status_mapped'] = df['loan_status'].map(mapping)


# Display the updated dataframe
df[['loan_status','loan_status_general', 'loan_status_mapped']].sample(10)

# Performing label encoding
df['grade_mapped'] = df['grade'].map({'A': 4, 'B': 3, 'C': 2, 'D': 1, 'E': 1, 'F': 1, 'G': 1})

# Creating a dictionary for mapping
label_mapping = {
    4: 'Very Good Debt',
    3: 'Low Risk Debt',
    2: 'Medium Risk Debt',
    1: 'High Risk Debt'
}

# Replacing the encoded values with the desired labels
df['grade_label'] = df['grade_mapped'].map(label_mapping)

# Displaying the results
df[['grade', 'grade_mapped', 'grade_label']]
## Loan analysis by grade

# Define the mapping dictionary
mapping = {
    'Verified': 1,
    'Source Verified': 1,
    'Not Verified': 0
}

# Create a new column 'verification_status_mapped' with the mapped values
df['verification_status_mapped'] = df['verification_status'].map(mapping)

# Display the updated dataframe
df[['verification_status', 'verification_status_mapped']].sample(5)

# Create a new column 'home_ownership_mapped' with the updated values
df['home_ownership'] = df['home_ownership'].replace({'NONE': 'OTHER', 'ANY': 'OTHER'})

# Display the updated dataframe
df['home_ownership'].unique()

# Perform one-hot encoding on 'home_ownership' column
one_hot_encoded = pd.get_dummies(df['home_ownership'], prefix='home_ownership')

# Concatenate the one-hot encoded DataFrame with the original DataFrame
df_encoded = pd.concat([df, one_hot_encoded], axis=1)

# Get the column names of the encoded columns
encoded_columns = df_encoded.columns[df.shape[1]:]

# Display the column names
encoded_columns = encoded_columns.tolist()
encoded_columns

"""## Feature Selection"""

# Choosen Columns
selected_columns = [
    'loan_status_mapped', 'grade_mapped','installment_vs_monthly_inc_ratio', 'verification_status_mapped','emp_length_encoded', 'collections_12_mths_ex_med','approval_status',
    'int_rate','installment','annual_inc','dti','delinq_2yrs','inq_last_6mths','open_acc',
    'revol_util','total_rec_late_fee','collection_recovery_fee','last_pymnt_amnt','tot_coll_amt',
    'tot_cur_bal','total_rev_hi_lim','loan_grade_score'] + encoded_columns

# Assuming your dataset is stored in a DataFrame called 'data'
duplicate_columns = df_encoded[selected_columns].columns[df_encoded[selected_columns].columns.duplicated()]
print(duplicate_columns)

"""## Splitting Data Into"""

# X is the dataframe containing the features (columns) of the data
X = df_encoded[selected_columns].drop('approval_status', axis=1)

# y is the dataframe containing the target (labels) of the data
y = df_encoded['approval_status']

"""## Standardization"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""# Analaysis"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Generate the word cloud
wordcloud = WordCloud().generate(' '.join(emp for emp in df.emp_title if pd.notnull(emp)))

# Create a figure and plot the word cloud
plt.figure(figsize=(10, 15))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")

# Display the word cloud
plt.show()

selected_columns =  ['grade','term', 'home_ownership', 'verification_status', 'pymnt_plan', 'initial_list_status', 'application_type','loan_grade']

# Determining the number of rows and columns for subplots
num_plots = len(selected_columns)
num_cols = 2
num_rows = (num_plots + num_cols - 1) // num_cols

# Creating subplots
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))

# Setting up a counter for plots
counter = 0

# Creating horizontal count plots for each object column
for row in range(num_rows):
    for col in range(num_cols):
        if counter < num_plots:
            col_name = selected_columns[counter]
            sns.countplot(data=df, y=col_name, ax=axes[row, col], orient='h', hue='loan_status_general')
            axes[row, col].set_title(f"Count Plot - {col_name}")
            counter += 1
        else:
            # If the counter exceeds the number of object columns, remove the remaining subplot
            fig.delaxes(axes[row, col])

# Adjusting the layout of subplots
plt.tight_layout()

# Displaying the plot
plt.show()

# Determining the number of rows and columns for subplots
num_plots = len(selected_columns)
num_cols = 2
num_rows = (num_plots + num_cols - 1) // num_cols

# Creating subplots
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 18))

# Setting up a counter for plots
counter = 0

# Creating horizontal count plots for each object column
for row in range(num_rows):
    for col in range(num_cols):
        if counter < num_plots:
            col_name = selected_columns[counter]
            ax = sns.countplot(data=df, x=col_name, ax=axes[row, col], hue='loan_status_general')
            ax.set_title(f"Count Plot - {col_name}")
            total = len(df[df[col_name].notna()])  # Exclude missing values
            for p in ax.patches:
                width = p.get_width()
                height = p.get_height()
                x, y = p.get_xy()
                percentage = height / total * 100
                ax.annotate(f'{percentage:.2f}%', (x + width/2, y + height), ha='center', va='bottom')
            counter += 1
        else:
            # If the counter exceeds the number of object columns, remove the remaining subplot
            fig.delaxes(axes[row, col])

# Adjusting the layout of subplots
plt.tight_layout()

# Displaying the plot
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

selected_columns = ['grade', 'verification_status', 'purpose']

num_plots = len(selected_columns)
num_cols = 2
num_rows = (num_plots + num_cols - 1) // num_cols

# Creating subplots
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 18))

# Setting up a counter for plots
counter = 0

# Creating horizontal count plots for each object column
for row in range(num_rows):
    for col in range(num_cols):
        if counter < num_plots:
            col_name = selected_columns[counter]
            ax = sns.countplot(data=df, y=col_name, ax=axes[row, col], hue='loan_status_general')
            ax.set_title(f"Count Plot - {col_name}")
            total = len(df[df[col_name].notna()])  # Exclude missing values
            for p in ax.patches:
                width = p.get_width()
                height = p.get_height()
                x, y = p.get_xy()
                percentage = width / total * 100
                ax.annotate(f'{percentage:.2f}%', (width, y + height / 2), va='center')
            ax.set_xlabel('Percentage')
            counter += 1
        else:
            # If the counter exceeds the number of object columns, remove the remaining subplot
            fig.delaxes(axes[row, col])

# Adjusting the layout of subplots
plt.tight_layout()

# Displaying the plot
plt.show()

"""# Modelling"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
from scipy.stats import ks_2samp
from sklearn.metrics import roc_auc_score

def train_test_evaluate_model(model, X_train, X_test, y_train, y_test):
    # Training the model
    model.fit(X_train, y_train)

    # Predicting the labels for test set
    y_pred = model.predict(X_test)

    # Calculating evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # Printing the evaluation metrics
    print("Training is success!")
    print("Accuracy: {:.2f}, Precision: {:.2f}, Recall: {:.2f}, F1-score: {:.2f}".format(
        accuracy, precision, recall, f1
    ))

    # Predicting probabilities for test set
    y_pred_proba = model.predict_proba(X_test)

    # Printing AUC, KS score, and classification report
    ks, roc_auc = evaluate_ks_and_roc_auc(y_test, y_pred_proba)
    matrix = classification_report(y_test, y_pred)
    print('Classification report:\n', matrix)

    # Generating and plotting confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    target_names = ["Approved", "Rejected"]
    plot_confusion_matrix(cm, target_names, title='Confusion Matrix', cmap=None, normalize=False)

    # Returning the evaluation metrics and the train/test sets
    return accuracy, precision, recall, f1, X_train, X_test, y_train, y_test

def evaluate_ks_and_roc_auc(y_real, y_proba):
    # Unite both visions to be able to filter
    df = pd.DataFrame()
    df['real'] = y_real
    df['proba'] = y_proba[:, 1]

    # Recover each class
    class0 = df[df['real'] == 0]
    class1 = df[df['real'] == 1]

    ks = ks_2samp(class0['proba'], class1['proba'])
    roc_auc = roc_auc_score(df['real'], df['proba'])

    print(f"ROC AUC: {roc_auc:.4f}")
    print(f"KS: {ks.statistic:.4f} (p-value: {ks.pvalue:.3e})")
    return ks.statistic, roc_auc

def plot_confusion_matrix(cm,
                          target_names,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True):
    import matplotlib.pyplot as plt
    import numpy as np
    import itertools

    accuracy = np.trace(cm) / np.sum(cm).astype('float')
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Blues')

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")


    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
    plt.show()

"""## Logistic Regression"""

from sklearn.linear_model import LogisticRegression

# Define the modell
model_logistic = LogisticRegression(max_iter=1000)

# Training, testing, and evaluation model
accuracy, precision, recall, f1, _, _, _, _ = train_test_evaluate_model(model_logistic, X_train, X_test, y_train, y_test)

from sklearn.ensemble import GradientBoostingClassifier

# Inisialisasi model Gradient Boosting Classifier
model_GBC = GradientBoostingClassifier()

# Train, test, dan evaluasi model Gradient Boosting Classifier
accuracy, precision, recall, f1, _, _, _, _ = train_test_evaluate_model(model_GBC, X_train, X_test, y_train, y_test)

"""## Random Forest"""

from sklearn.ensemble import RandomForestClassifier

# Initialize the Random Forest model
model_RF = RandomForestClassifier(random_state=42)

# Train, test, and evaluate the model
accuracy, precision, recall, f1, _, _, _, _ = train_test_evaluate_model(model_RF, X_train, X_test, y_train, y_test)

"""## Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

# Inisialisasi model Naive Bayes
model_nb = GaussianNB()

# Latih, uji, dan evaluasi model
accuracy, precision, recall, f1, _, _, _, _ = train_test_evaluate_model(model_nb, X_train, X_test, y_train, y_test)

"""## Multinomial Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB

# Define the model
model_mnb = MultinomialNB()

# Latih, uji, dan evaluasi model
accuracy, precision, recall, f1, _, _, _, _ = train_test_evaluate_model(model_mnb, X_train, X_test, y_train, y_test)

from sklearn.ensemble import VotingClassifier

# Inisialisasi model Logistic Regression
model_LR = LogisticRegression(solver='lbfgs',max_iter=1000)

# Inisialisasi model Multinomial Naive Bayes
model_MNB = MultinomialNB()

# Inisialisasi model Naive Bayes
model_nb = GaussianNB()

# Inisialisasi model Voting Classifier
voting_model = VotingClassifier(
    estimators=[('lr', model_LR), ('mnb', model_MNB), ('nb', model_nb)],
    voting='soft'
)

# Training, testing, dan evaluasi model Voting Classifier
accuracy, precision, recall, f1, _, _, _, _ = train_test_evaluate_model(voting_model, X_train, X_test, y_train, y_test)

"""# Tuning Hyperparameter"""

from sklearn.linear_model import LogisticRegression

# Define the model
model_logistic = LogisticRegression(solver='lbfgs', max_iter=2000, penalty='l2', C=0.38)

# Training, testing, and evaluation model
accuracy, precision, recall, f1, _, _, _, _ = train_test_evaluate_model(model_logistic, X_train, X_test, y_train, y_test)

import matplotlib.pyplot as plt

# Get feature coefficients
feature_importance = abs(model_logistic.coef_[0])

# Get feature names
feature_names = selected_columns[:len(feature_importance)]

# Combine features and their importance values
feature_importance_sorted = sorted(zip(feature_names, feature_importance), key=lambda x: x[1])

# Separate sorted feature names and importance values
sorted_feature_names, sorted_feature_importance = zip(*feature_importance_sorted)

# Create the plot
plt.figure(figsize=(10, 6))
plt.barh(sorted_feature_names, sorted_feature_importance)
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.title("Feature Importance - Logistic Regression")
plt.show()

